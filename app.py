
import fitz
import numpy as np
import streamlit as st
from groq import Groq
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Ø¥Ø¹Ø¯Ø§Ø¯ Groq
client = Groq(api_key=st.secrets["GROQ_API_KEY"])

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = SentenceTransformer("all-MiniLM-L6-v2")

# ØªØ­Ù…ÙŠÙ„ PDF ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù†Øµ
@st.cache_data
def load_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Chunks
def split_text_into_chunks(text, chunk_size=5):
    import re
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [" ".join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]

# Ø¥Ù†Ø´Ø§Ø¡ Embeddings
@st.cache_resource
def embed_chunks(chunks):
    return model.encode(chunks, show_progress_bar=False)

# Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† chunks
def retrieve_chunks(query, chunks, chunk_embeddings, top_n=5):
    query_embedding = model.encode([query])[0]
    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]
    top_indices = np.argsort(similarities)[::-1][:top_n]
    return [(chunks[i], similarities[i]) for i in top_indices]

# ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Groq
def generate_answer(context_chunks, question):
    context = "\n".join([f"- {chunk}" for chunk, _ in context_chunks])
    prompt = f"""You are a helpful assistant.
Use only the following context to answer the user's question. Do not invent or assume any information.

Context:
{context}

Question: {question}
Answer:"""
    response = client.chat.completions.create(
        model="llama3-8b-8192",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return response.choices[0].message.content

# ======================= ÙˆØ§Ø¬Ù‡Ø© Streamlit =========================
st.set_page_config(page_title="RAG PDF Chatbot", layout="wide")
st.title("ðŸ¤– Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ù„Ø§Ø¦Ø­Ø© Ø­ÙˆÙƒÙ…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (NDMO)")

pdf_path = "ndmo_en.pdf"
with st.spinner("ðŸ“„ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©..."):
    text = load_pdf_text(pdf_path)
    chunks = split_text_into_chunks(text)
    embeddings = embed_chunks(chunks)

st.success("âœ… ØªÙ… ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Ø¸Ø§Ù…! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.")

question = st.text_input("â“ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:", placeholder="Ù…Ø«Ø§Ù„: What are the objectives?")
if question:
    with st.spinner("ðŸ’¡ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
        top_chunks = retrieve_chunks(question, chunks, embeddings)
        answer = generate_answer(top_chunks, question)
        st.markdown("### ðŸ“Œ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
        st.write(answer)

        with st.expander("ðŸ“š Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©"):
            for i, (chunk, score) in enumerate(top_chunks, 1):
                st.markdown(f"""**ðŸ”¹ Ø§Ù„Ù‚Ø·Ø¹Ø© {i} (Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {score:.2f})**  
{chunk}""")


{chunk}")
