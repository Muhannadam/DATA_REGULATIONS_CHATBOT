
import fitz
import numpy as np
import streamlit as st
from groq import Groq
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Ø¥Ø¹Ø¯Ø§Ø¯ Groq
client = Groq(api_key=st.secrets["GROQ_API_KEY"])
@st.cache_data(ttl=3600)
def list_groq_models():
    models = client.models.list()
    return [m.id for m in models.data]

with st.sidebar:
    st.write("Available Groq models:")
    st.write(list_groq_models())


# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = SentenceTransformer("all-MiniLM-L6-v2")

# ØªØ­Ù…ÙŠÙ„ PDF ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù†Øµ
@st.cache_data
def load_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Chunks
def split_text_into_chunks(text, chunk_size=5):
    import re
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [" ".join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]

# Ø¥Ù†Ø´Ø§Ø¡ Embeddings
@st.cache_resource
def embed_chunks(chunks):
    return model.encode(chunks, show_progress_bar=False)

# Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† chunks
def retrieve_chunks(query, chunks, chunk_embeddings, top_n=5):
    query_embedding = model.encode([query])[0]
    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]
    top_indices = np.argsort(similarities)[::-1][:top_n]
    return [(chunks[i], similarities[i]) for i in top_indices]

# ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Groq
def generate_answer(context_chunks, question):
    context = "\n".join([f"- {chunk}" for chunk, _ in context_chunks])
    prompt = f"""You are a helpful assistant.
Use only the following context to answer the user's question. Do not invent or assume any information.

Context:
{context}

Question: {question}
Answer:"""
    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return response.choices[0].message.content

# ======================= ÙˆØ§Ø¬Ù‡Ø© Streamlit =========================
st.set_page_config(page_title="RAG PDF Chatbot", layout="wide")
st.title("By Muhannad Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ù„Ø§Ø¦Ø­Ø© Ø­ÙˆÙƒÙ…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (NDMO)")

pdf_path = "ndmo_en.pdf"
with st.spinner("ðŸ“„ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©..."):
    text = load_pdf_text(pdf_path)
    chunks = split_text_into_chunks(text)
    embeddings = embed_chunks(chunks)

st.success("âœ… ØªÙ… ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Ø¸Ø§Ù…! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.")

# Ø£Ø³Ø¦Ù„Ø© Ù…Ù‚ØªØ±Ø­Ø© Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
suggestions = [
    "What are the key objectives of the National Data Governance Interim Regulations?",
    "What is the scope of data classification in the interim regulations?",
    "Who is responsible for ensuring compliance with data privacy rules?",
]

# Ø¹Ø±Ø¶ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
st.markdown("### âœ¨ Ø¬Ø±Ù‘Ø¨ Ø£Ø­Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©:")

# Ù…ØªØºÙŠØ± Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø®ØªØ§Ø±
default_question = ""

# Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø²Ø±Ø§Ø± ÙÙŠ ØµÙ Ø£ÙÙ‚ÙŠ
cols = st.columns(len(suggestions))
for i, q in enumerate(suggestions):
    if cols[i].button(f"ðŸ’¬ {q[:80]}..."):  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 40 Ø­Ø±Ù ÙÙ‚Ø· Ù„ØªØµØºÙŠØ± Ø§Ù„Ø²Ø±
        default_question = q

# Ø­Ù‚Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¹ ØªØ¹Ø¨Ø¦Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ø¥Ø°Ø§ ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø²Ø±
question = st.text_input("â“ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:", value=default_question, placeholder="Ù…Ø«Ø§Ù„: What is data governance?")

if question:
    with st.spinner("ðŸ’¡ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
        top_chunks = retrieve_chunks(question, chunks, embeddings)
        answer = generate_answer(top_chunks, question)
        st.markdown("### ðŸ“Œ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
        st.write(answer)

        with st.expander("ðŸ“š Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©"):
            for i, (chunk, score) in enumerate(top_chunks, 1):
                st.markdown(f"""**ðŸ”¹ Ø§Ù„Ù‚Ø·Ø¹Ø© {i} (Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {score:.2f})**

{chunk}""")
